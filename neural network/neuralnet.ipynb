{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims= struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        \n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(image_path, label_path):\n",
    "    images = read_idx(image_path)\n",
    "    labels = read_idx(label_path)\n",
    "    return images, labels\n",
    "\n",
    "train_image_path = r'input\\train-images.idx3-ubyte'\n",
    "train_label_path = r'input\\train-labels.idx1-ubyte'\n",
    "test_image_path =  r'input\\t10k-images.idx3-ubyte'\n",
    "test_label_path =  r'input\\t10k-labels.idx1-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_images, train_labels = load_mnist(train_image_path, train_label_path)\n",
    "test_images, test_labels = load_mnist(test_image_path, test_label_path)\n",
    "print(f'Train images shape: {train_images.shape}')\n",
    "print(f'Train labels shape: {train_labels.shape}')\n",
    "print(f'Test images shape: {test_images.shape}')\n",
    "print(f'Test labels shape: {test_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, no_of_neurons, input_no, activation):\n",
    "        self.weights = np.random.rand(input_no, no_of_neurons) - 0.5\n",
    "        self.bias = np.random.rand(1, no_of_neurons) - 0.5\n",
    "        self.activation = activation\n",
    "        self.z = None\n",
    "        self.out_vals = None\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_para, layers):\n",
    "        self.ListOfLayers = []\n",
    "        prev_no = input_para\n",
    "\n",
    "        for layer in layers:\n",
    "            temp_obj = Layer(layer[0], prev_no, layer[1])\n",
    "            self.ListOfLayers.append(temp_obj)\n",
    "            prev_no = layer[0]\n",
    "\n",
    "    def apply_activation(self, values, activation_fun):\n",
    "        if activation_fun == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif activation_fun == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif activation_fun == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif activation_fun == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def apply_activation_derivative(self, values, activation_fun):\n",
    "        if activation_fun == 'relu':\n",
    "            return np.where(values > 0, 1, 0)\n",
    "        elif activation_fun == 'sigmoid':\n",
    "            sigmoid = 1 / (1 + np.exp(-values))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        elif activation_fun == 'tanh':\n",
    "            return 1 - np.tanh(values) ** 2\n",
    "        else:\n",
    "            return np.ones_like(values)  # No activation (identity)\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01,printEvery=1):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                self.Backpropagation(X[i], y[i], learning_rate)\n",
    "            if epoch % printEvery == 0:\n",
    "                loss = self.compute_loss(X, y)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        m = len(X)\n",
    "        total_loss = 0\n",
    "        for i in range(m):\n",
    "            output = self.forward(X[i])\n",
    "            total_loss -= np.log(output[0, y[i]])\n",
    "        return total_loss / m\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for layer in self.ListOfLayers:\n",
    "            pre = np.dot(output, layer.weights) + layer.bias\n",
    "            output = self.apply_activation(pre, layer.activation)\n",
    "            layer.z = pre\n",
    "            layer.out_vals = output\n",
    "        return output\n",
    "\n",
    "    def Backpropagation(self, x, y, learning_rate):\n",
    "        m = 1  # stochastic gradient descent \n",
    "        self.forward(x)\n",
    "\n",
    "        # Output layer error\n",
    "        delta = self.ListOfLayers[-1].out_vals - np.eye(len(self.ListOfLayers[-1].out_vals[0]))[y]\n",
    "        self.ListOfLayers[-1].delta = delta\n",
    "\n",
    "        # Backward pass\n",
    "        for i in reversed(range(len(self.ListOfLayers) - 1)):\n",
    "            layer = self.ListOfLayers[i]\n",
    "            next_layer = self.ListOfLayers[i + 1]\n",
    "            delta = np.dot(next_layer.delta, next_layer.weights.T) * self.apply_activation_derivative(layer.z, layer.activation)\n",
    "            layer.delta = delta\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.ListOfLayers)):\n",
    "            layer = self.ListOfLayers[i]\n",
    "            a_prev = np.atleast_2d(x if i == 0 else self.ListOfLayers[i - 1].out_vals)\n",
    "            layer.weights -= learning_rate * np.dot(a_prev.T, layer.delta) / m\n",
    "            layer.bias -= learning_rate * np.sum(layer.delta, axis=0, keepdims=True) / m\n",
    "\n",
    "    def addCustomWeights(self, weights, bias, ind):\n",
    "        self.ListOfLayers[ind].weights = weights\n",
    "        self.ListOfLayers[ind].bias = bias\n",
    "\n",
    "    def WeightShape(self):\n",
    "        for i, layer in enumerate(self.ListOfLayers):\n",
    "            print(f'weight{i} - {layer.weights.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Reshape train and test images\n",
    "train_images = train_images.reshape((train_images.shape[0], 784))\n",
    "test_images = test_images.reshape((test_images.shape[0], 784))\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(784,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.35439571961563665\n",
      "Epoch 1, Loss: 0.28293043751114016\n",
      "Epoch 2, Loss: 0.2805918929513196\n",
      "Epoch 3, Loss: 0.23798009708583917\n",
      "Epoch 4, Loss: 0.22582139560457132\n",
      "Epoch 5, Loss: 0.24428329802722393\n",
      "Epoch 6, Loss: 0.23048055662673356\n",
      "Epoch 7, Loss: 0.21181354367324415\n",
      "Epoch 8, Loss: 0.1966570743519448\n",
      "Epoch 9, Loss: 0.1977799976971104\n"
     ]
    }
   ],
   "source": [
    "layers = [(16,'relu'),(16,'relu'),(10,'softmax')]\n",
    "\n",
    "mymodel = NeuralNetwork(784,layers)\n",
    "mymodel.train(train_images, train_labels, epochs=10, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for weights and shit\n",
    "# for layer_index, keras_layer in enumerate(model.layers):\n",
    "#     weights, biases = keras_layer.get_weights()\n",
    "#     print(weights.shape) # debugging\n",
    "#     mymodel.addCustomWeights(weights, biases, layer_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mymodel.WeightShape()\n",
    "lable = train_labels[76]\n",
    "in_img = train_images[76].reshape(1,784)\n",
    "print(lable)\n",
    "np.argmax(mymodel.forward(in_img))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
